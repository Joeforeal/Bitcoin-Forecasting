---
title: "Project 3"
author: "Sia Gao, Marc Luzuriaga, Reagan Lee, Jiaxuan Huang"
date: "2024-12-06"
output: pdf_document
---

```{R library, include = F, echo = F, warning = F, message = F}
library(quantmod)
library(forecast)
library(tseries)
library(ggplot2)
library(prophet)
```

# Introduction

Bitcoin, the world's first decentralized cryptocurrency, has become a focal point of interest for financial analysts, data scientists, and economists. Since its inception in 2009, Bitcoin has witnessed a meteoric rise in popularity and valuation, making it a significant subject of study in the realm of financial forecasting. Forecasting Bitcoin returns is particularly challenging due to its inherent volatility, speculative nature, and sensitivity to global economic events.

This project aims to develop and compare various time series forecasting models to predict Bitcoin returns, leveraging the robust tools available in R. The dataset, obtained from the R package's Bitcoin data, provides historical values of Bitcoin prices, which will be transformed into returns for analysis. By examining and refining different models, including ARIMA, ETS, Holt-Winters, NNETAR, Prophet, and a combination of these methods, the study will try to identify the best forecasting approach based on training and testing errors. Insights from this analysis can contribute to a better understanding of Bitcoin's price dynamics and aid in financial decision-making.

# Data

For this project, instead of manually download data from the website, we're importing Bitcoin data directly using the `quantmod` package.

```{R, include = T, echo = T, warning = F, message = F}
# Fetch historical Bitcoin data from Yahoo Finance
getSymbols("BTC-USD", src = "yahoo", from = "2010-01-01", to = Sys.Date())

# Use the closing price of Bitcoin and omit missing values
bitcoin_prices <- na.omit(Cl(`BTC-USD`))

# Convert to time series object with the daily frequency
bitcoin_ts <- ts(bitcoin_prices, frequency = 365) 

# Plot the original Bitcoin Prices data
tsdisplay(bitcoin_ts, main = "Original Bitcoin Prices")
```

From the graph of bitcoin prices, we observed that the data has an obvious upward trend with some potential cycles. However, since most of the models prefer stationary data and our research questions is focus on the returns of the bitcoin prices in order to observe the dynamic of market, we are now taking the difference to get the returns and check for the staionarity.

```{R, include = T, echo = T, warning = F, message = F}
bitcoin_returns <- diff(log(bitcoin_prices))
bitcoin_returns <- na.omit(bitcoin_returns)

# Plot returns
tsdisplay(bitcoin_returns, main = "Bitcoin Returns")
```

Compared with the Bitcoin prices graph, the Bitcoin returns are much more similar to the white noises; however, there are still some spikes in the ACF and PACF, so we need to use some further official methods - ADF test - to determine the stationarity of the Bitcoin Returns Data.

```{R, include = T, echo = T, warning = F, message = F}
# Test for stationarity of the returns
adf_test_returns <- adf.test(bitcoin_returns)
print(adf_test_returns)
```

From the p-value of the Augmented Dickey-Fuller Test, we found a significant result, which allows us to reject the null hypothesis and conclude that the Bitcoin returns data is stationary.

After confirming the stationarity of the data, we are now splitting the data into training and testing sets to forecast using the same dataset for all models, allowing for a horizontal comparison.

```{R, include = T, echo = T, warning = F, message = F}
# Define the training set size as 80% of the total dataset
train_size <- floor(0.8 * length(bitcoin_returns))

# Split the data into training (first 80%) and testing (last 20%)
train_data <- bitcoin_returns[1:train_size]  # First 80%
test_data <- bitcoin_returns[(train_size + 1):length(bitcoin_returns)]# Last 20%

# Define the forecasting horizon (length of the test set)
forecast_horizon <- length(test_data)

# # Combine all data (training + testing) into a single time series
all_data <- ts(c(train_data, test_data), start = start(bitcoin_returns), 
               frequency = frequency(bitcoin_returns))

# Convert training data to a time series
train_data <- ts(train_data, start = start(all_data), 
                 frequency = frequency(all_data))

# Convert testing data to a time series
test_data <- ts(test_data, start = time(all_data)[train_size + 1], 
                frequency = frequency(all_data))
```

Now, the data has been split into 80% for training and the remianing 20% for testing. We're going to use `train_data` and `test_data` in order to build models and test for the models.

# Results

## ARIMA

For this step, we're going to build the arima model by using the training set and then forecast the testing set part, which has a forecast_horizon of 20% of the dataset.

```{R, include = T, echo = T, warning = F, message = F}
# Fit the ARIMA model to the training data
arima_model <- auto.arima(train_data)

# Print the ARIMA model summary
summary(arima_model)
```

From the summary of the arima model, we noticed that the model includes two autoregressive terms (AR) and no moving average terms (MA). Since the data is already stationary, no differencing should be applied.

The weak AR coefficients suggest that past values (lags) have limited influence on the current values. - ar1 = -0.0195: Indicates a weak, slightly negative influence of the first lag on the current value. - ar2 = 0.0064: Indicates an even weaker positive influence of the second lag.

```{R, include = T, echo = T, warning = F, message = F}
# Test for AIC and BIC
arima_aic <- AIC(arima_model)
arima_bic <- BIC(arima_model)
cat("The AIC of the arima model is: ", 
    arima_aic, "; and the BIC is: ", arima_bic, ".")
```

Since the AIC and BIC are necessary when choosing a final model, we're listing the AIC and BIC above for further comparsions.

After building the model, let's forecast it:

```{R, include = T, echo = T, warning = F, message = F}
# Generate forecasts for the testing period
arima_forecast <- forecast(arima_model, h = forecast_horizon)

# Convert ARIMA forecast to a time series aligned with the testing set
arima_forecast <- ts(arima_forecast$mean, 
                        start = time(test_data)[1], 
                        frequency = frequency(all_data))

# Plot training data, testing data, and ARIMA forecast
autoplot(train_data, series = "Training Data") +
  autolayer(test_data, series = "Testing Data") +
  autolayer(arima_forecast, series = "ARIMA Forecast") +
  ggtitle("ARIMA Forecast with Training and Testing Data") +
  ylab("Returns") +
  xlab("Time") +
  theme_minimal() +
  scale_color_manual(values = c("red", "grey", "black")) +
  labs(color = "Series")
```

From the plots, the ARIMA forecast is a horizontal line. It is reasonable since the data is stationary and the validity is smaller compared to the training set validity.

```{R, include = T, echo = T, warning = F, message = F}
# Calculate accuracy metrics for ARIMA
arima_accuracy <- accuracy(arima_forecast, test_data)

# Display the accuracy metrics
print(arima_accuracy)
```

The accuracy metrics for the ARIMA model reveal its performance characteristics. The small Mean Error (ME) of 0.00116 suggests a slight positive bias, while the Root Mean Squared Error (RMSE) of 0.02498 and Mean Absolute Error (MAE) of 0.01739 indicate relatively low forecast errors, with RMSE penalizing larger errors more heavily. The unusually high Mean Percentage Error (MPE) and Mean Absolute Percentage Error (MAPE) values (140.80% and 173.84%) are likely inflated by near-zero actual values in the test set. The minimal Autocorrelation of Residuals (ACF1) at -0.02708 is a positive sign, indicating no significant correlation in residuals. However, the Theilâ€™s U statistic of 1.046 suggests the model's performance is only slightly better than a naÃ¯ve forecast.

Examining residuals is a critical step in time series analysis to validate model assumptions and ensure its adequacy. Residuals should ideally resemble white noise, meaning they are uncorrelated, normally distributed, and have constant variance over time. Any patterns or significant autocorrelation in residuals could indicate that the model fails to capture important structures in the data. To further assess the ARIMA model's validity, we will now evaluate its residuals through diagnostic plots and statistical tests.

```{R, include = T, echo = T, warning = F, message = F}
# Check residual diagnostics for the ARIMA model
checkresiduals(arima_model)
```

Based on the Ljung-Box test, since the p-value (0.02639) is less than the 0.05 significance level, we reject the null hypothesis. This indicates that the residuals exhibit some level of autocorrelation and are not purely white noise. This could mean that the ARIMA(2,0,0) model fails to capture all patterns in the data, suggesting room for improvement.

From the Histogram of Residuals (Bottom Right), we observed a slight deviation from normality, with a sharp peak at the center and slightly heavier tails. This indicates that the model may struggle with extreme observations or volatility in the data.

After evaluating the ARIMA model, we will next explore the ETS (Error, Trend, Seasonality) model, which is well-suited for capturing exponential smoothing patterns in time series data. Unlike ARIMA, the ETS model explicitly handles trend and seasonality components. This makes it particularly effective for modeling data with clear level shifts or seasonal patterns, providing an alternative perspective on Bitcoin returns.

## ETS (Reagan)
```{R, include = T, echo = T, warning = F, message = F}
# Fit the ETS model to the training data
ets_model <- ets(train_data)

# Print the ETS model summary
summary(ets_model)
```
The ETS(A,N,N) model effectively captures the stationary nature of Bitcoin returns by assuming additive error, no trend, and no seasonality. With a low smoothing parameter (ð›¼=0.0033) and minimal residual autocorrelation (ACF1 = -0.0224), the model demonstrates reliable performance, though its simplicity may limit its ability to handle Bitcoin's high volatility.

```{R, include = T, echo = T, warning = F, message = F}
# Generate forecasts for the testing period
ets_forecast <- forecast(ets_model, h = forecast_horizon)

# Convert ETS forecast to a time series aligned with the testing set
ets_forecast_ts <- ts(ets_forecast$mean, 
                      start = time(test_data)[1], 
                      frequency = frequency(all_data))

# Plot training data, testing data, and ETS forecast
autoplot(train_data, series = "Training Data") +
  autolayer(test_data, series = "Testing Data") +
  autolayer(ets_forecast_ts, series = "ETS Forecast") +
  ggtitle("ETS Forecast with Training and Testing Data") +
  ylab("Returns") +
  xlab("Time") +
  theme_minimal() +
  scale_color_manual(values = c("red", "grey", "blue")) +
  labs(color = "Series")
```
The ETS(A,N,N) model produces a stable forecast (red line) that aligns with the stationary nature of Bitcoin returns, while the actual testing data (grey line) demonstrates significant volatility. This highlights the model's inability to fully capture the unpredictable fluctuations of Bitcoin returns, as it smooths over the extreme variations observed in the testing data.

```{R, include = T, echo = T, warning = F, message = F}
# Calculate accuracy metrics for ETS
ets_accuracy <- accuracy(ets_forecast_ts, test_data)

# Display the accuracy metrics
print(ets_accuracy)
```
The ETS(A,N,N) model shows reasonable performance with a low RMSE (0.0252) and MAE (0.0175), indicating moderate forecasting accuracy for Bitcoin returns. However, the high MAPE (193.12%) and MPE (46.57%) reveal challenges in handling extreme volatility and small values in the testing dataset, reflecting the limitations of the model in highly erratic financial markets.

```{R, include = T, echo = T, warning = F, message = F}
# Check residual diagnostics for the ETS model
checkresiduals(ets_model)
```
The residual diagnostics for the ETS(A,N,N) model show that residuals are centered around zero with no significant autocorrelation, as confirmed by the Ljung-Box test (p-value = 0.05839). However, some residual spikes and slight deviations from normality suggest that the model does not fully capture all dynamics, especially during volatile periods.

## Holt-Winters (Reagan)

```{R, include = T, echo = T, warning = F, message = F}
# Fit the Holt-Winters model to the training data
holtwinters_model <- HoltWinters(train_data, beta = FALSE, gamma = FALSE)

# Print the Holt-Winters model summary
summary(holtwinters_model)
```
The Holt-Winters model applied to the training data used a smoothing parameter (alpha) to effectively capture the stationary behavior of Bitcoin returns, while excluding trend (beta=FALSE) and seasonality (gamma=FALSE). The model's results, including fitted values and a calculated Sum of Squared Errors (SSE), demonstrate its ability to produce stable and reliable forecasts for the volatile returns dataset.

```{R, include = T, echo = T, warning = F, message = F}
# Generate forecasts for the testing period
holtwinters_forecast <- forecast(holtwinters_model, h = forecast_horizon)

# Convert Holt-Winters forecast to a time series aligned with the testing set
holtwinters_forecast_ts <- ts(holtwinters_forecast$mean, 
                              start = time(test_data)[1], 
                              frequency = frequency(all_data))

# Plot training data, testing data, and Holt-Winters forecast
autoplot(train_data, series = "Training Data") +
  autolayer(test_data, series = "Testing Data") +
  autolayer(holtwinters_forecast_ts, series = "Holt-Winters Forecast") +
  ggtitle("Holt-Winters Forecast with Training and Testing Data") +
  ylab("Returns") +
  xlab("Time") +
  theme_minimal() +
  scale_color_manual(values = c("red", "grey", "purple")) +
  labs(color = "Series")


```
The Holt-Winters forecast (red line) produces a stable and nearly horizontal prediction aligned with the stationary nature of Bitcoin returns, capturing the level dynamics of the training data. While the forecast appears less volatile than the testing data (gray line), it highlights the model's inability to fully adapt to extreme fluctuations in the returns.

```{R, include = T, echo = T, warning = F, message = F}
# Calculate accuracy metrics for Holt-Winters
holtwinters_accuracy <- accuracy(holtwinters_forecast_ts, test_data)

# Display the accuracy metrics
print(holtwinters_accuracy)
```
The accuracy metrics for the Holt-Winters model indicate a Mean Error (ME) of 0.007, suggesting a slight positive bias, and a Root Mean Squared Error (RMSE) of 0.0259, reflecting moderate forecast errors. However, the high Mean Absolute Percentage Error (MAPE) of 411.84% highlights the model's struggle to handle the extreme volatility in Bitcoin returns, despite an overall reasonable performance as indicated by Theil's U statistic of 0.94, which shows it outperforms a naÃ¯ve model.

```{R, include = T, echo = T, warning = F, message = F}
# Check residual diagnostics for the Holt-Winters model
holtwinters_residuals <- test_data - holtwinters_forecast_ts

# Plot residuals
autoplot(holtwinters_residuals) +
  ggtitle("Residuals of Holt-Winters Model") +
  ylab("Residuals") +
  xlab("Time") +
  theme_minimal()

# Check residual diagnostics
checkresiduals(holtwinters_model)
```
The residual analysis of the Holt-Winters model shows that the residuals fluctuate around zero, indicating no significant bias in the forecast. The autocorrelation function (ACF) plot reveals minor autocorrelation at specific lags, but most values remain within the confidence bounds, suggesting that the model captures the overall structure of the data. The Ljung-Box test yields a p-value of 0.06074, which is slightly above the 0.05 threshold, indicating that the residuals are approximately uncorrelated and resemble white noise. However, the presence of outliers and deviations in the histogram of residuals suggests that the model struggles with extreme values in the data.

## NNETAR (Jiaxuan)

We will proceed to fit a NNETAR model to our data as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Fit the NNETAR model to the training data
nnetar_model <- nnetar(train_data)

# Print the NNETAR model summary
summary(nnetar_model)
```

From model summary, the NNETAR model uses a single hidden layer with basic lag parameters and produces forecasts by averaging results from multiple models to capture non-linear patterns in the data.

We will proceed to forecast as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Generate forecasts for the testing period using NNETAR
nnetar_forecast <- forecast(nnetar_model, h = forecast_horizon)

# Convert NNETAR forecast to a time series aligned with the testing set
nnetar_forecast <- ts(nnetar_forecast$mean, 
                      start = time(test_data)[1], 
                      frequency = frequency(all_data))

# Plot training data, testing data, and NNETAR forecast
autoplot(train_data, series = "Training Data") +
  autolayer(test_data, series = "Testing Data") +
  autolayer(nnetar_forecast, series = "NNETAR Forecast") +
  ggtitle("NNETAR Forecast with Training and Testing Data") +
  ylab("Returns") +
  xlab("Time") +
  theme_minimal() +
  scale_color_manual(values = c("red", "grey", "blue")) +
  labs(color = "Series")
```

From the plots, the NNETAR forecast is a horizontal line. It is reasonable since the data is stationary, seeming ly less volatile than test set.

Model preformance validation (AIC/BIC omitted since unsupported by neural networks)

```{R, include = T, echo = T, warning = F, message = F}
# Calculate accuracy metrics for NNETAR
nnetar_accuracy <- accuracy(nnetar_forecast, test_data)

# Display the accuracy metrics
print(nnetar_accuracy)
```

The accuracy metrics for the ARIMA model show an RMSE of 0.0249 and an MAE of 0.0174, indicating reasonable predictive accuracy. The Mean Error is quite low at 0.0013, suggesting minimal bias in the forecasts. However, the Mean Percentage Error and Mean Absolute Percentage Error are relatively high, indicating that there are substantial relative errors, which may be due to high volatility in the data. The acf of 0.027 is higher than the original data, suggesting weaker stationarity Theil's U statistic, close to 1, suggests that the model performs similarly to a naive benchmark model.

We will proceed to check the residuals as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Check residual diagnostics for the NNETAR model
checkresiduals(nnetar_model)
```

The residual diagnostics for the NNETAR model show that the residuals are mostly centered around zero, indicating that the model has captured the general trend of the data well. The autocorrelation plot reveals that most lags fall within the confidence bounds, suggesting minimal autocorrelation in the residuals, which implies that the model effectively accounted for the time series dynamics. However, there are a few spikes in the autocorrelation function that indicate some remaining dependencies. Overall, the residuals appear approximately normally distributed, although there is some minor deviation.

## Prophet (Jiaxuan)

We will proceed to fit a prophet model to our data as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Fit the Prophet model to the training data
train_df <- data.frame(ds = time(train_data), y = as.numeric(train_data))
prophet_model <- prophet(train_df)
summary(prophet_model)
```

The Prophet model summary indicates key components, such as growth for trend type, changepoints to handle structural changes, and multiple seasonality options (yearly, weekly, daily). It also includes attributes like holidays and various priors for regularization. These attributes provide control over trend flexibility, seasonal components, and other time series characteristics, making Prophet versatile for capturing complex seasonality and trend patterns in time series data.

We will proceed to perform the forecast as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Perform step-ahead forecasting with Prophet
prophet_forecast = predict(prophet_model, periods = len(test_data)) #forecast the length of test, since it does not handle ts object here
#here I am not exactly sure why generates almost 3000 even if I did periods = 746

# Extract only the forecasted values
prophet_forecast_values <- prophet_forecast$yhat

# Convert Prophet forecast to a time series aligned with the testing set
prophet_forecast_ts <- ts(prophet_forecast_values[1:746], 
                          start = time(test_data)[1], 
                          frequency = frequency(all_data))

# Plot training data, testing data, and Prophet forecast
autoplot(train_data, series = "Training Data") +
  autolayer(test_data, series = "Testing Data") +
  autolayer(prophet_forecast_ts, series = "Prophet Forecast") +
  ggtitle("Prophet Step-Ahead Forecast with Training and Testing Data") +
  ylab("Returns") +
  xlab("Time") +
  theme_minimal() +
  scale_color_manual(values = c("red", "grey", "green")) +
  labs(color = "Series")
```

Again, the prediction is nearly horizontal, and seemingly less volatile to test set, not very sure why it while required.

Let us assess the accuracy of or model as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Calculate accuracy metrics for Prophet
prophet_accuracy <- accuracy(prophet_forecast_ts, test_data)

# Display the accuracy metrics
print(prophet_accuracy)
```

The accuracy metrics for the Prophet model show a mean error of approximately 0.0017 and a root mean square error of 0.0251, indicating a relatively small error between predicted and actual values. The mean absolute error is around 0.0174, while the mean percentage error and mean absolute percentage error are 97.39% and 138.93%, respectively, suggesting some bias and magnitude of forecasting error. Theil's U statistic is approximately 1.007, which indicates that the forecasting performance is similar to using a naive model.

We will proceed to check the residuals:

```{R, include = T, echo = T, warning = F, message = F}
# Calculate residuals for the Prophet model
prophet_residuals <- test_data - prophet_forecast_ts

# Plot residuals
autoplot(prophet_residuals) +
  ggtitle("Residuals of Prophet Model") +
  ylab("Residuals") +
  xlab("Time") +
  theme_minimal()

# Check residual diagnostics
checkresiduals(prophet_residuals)
```

The residual analysis of the Prophet model shows that residuals fluctuate around zero, indicating that the model generally captures the trend. However, some significant spikes suggest the presence of outliers or periods where the model struggled to capture the variability accurately. The autocorrelation function (ACF) plot shows some minor lags outside the confidence intervals, implying that some temporal structure might still be present in the residuals, meaning there is room for improvement in capturing dependencies. The histogram of residuals shows a roughly normal distribution, indicating that the residual errors are reasonably spread out, but the presence of skew or kurtosis needs further investigation.

## Combination (Marc)

Let us perform a combination fit by averaging the forecasted values of our values:

```{R, include = T, echo = T, warning = F, message = F}

# Create the combination forecast
Num_Models <- 3
prophetforecast_adjusted <- prophet_forecast$yhat[1:forecast_horizon]
Combination <- (arima_forecast+nnetar_forecast+prophetforecast_adjusted)/Num_Models

# Plot training data, testing data, and Combination forecast
autoplot(train_data, series = "Training Data") +
  autolayer(test_data, series = "Testing Data") +
  autolayer(Combination, series = "Combination Forecast") +
  ggtitle("Combination Step-Ahead Forecast with Training and Testing Data") +
  ylab("Returns") +
  xlab("Time") +
  theme_minimal() +
  scale_color_manual(values = c("red", "grey", "green")) +
  labs(color = "Series")
```

We will proceed to analyze the model performance as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Calculate accuracy metrics for Combination Forecast
combination_accuracy <- accuracy(Combination, test_data)

# Display the accuracy metrics
print(combination_accuracy)
```

We will proceed to check the residuals as follows:

```{R, include = T, echo = T, warning = F, message = F}
# Calculate residuals for the Combination model
combination_residuals <- test_data - Combination

# Plot residuals
autoplot(combination_residuals) +
  ggtitle("Residuals of Combination Model") +
  ylab("Residuals") +
  xlab("Time") +
  theme_minimal()

# Check residual diagnostics
checkresiduals(combination_residuals)
```

# Conclusion & Future Works

# References

Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole.
